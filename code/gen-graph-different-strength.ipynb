{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate graph for multiple benign and attacked imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set auto reload for python functions\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "# GoogLeNet\n",
    "import lucid.modelzoo.vision_models as models\n",
    "\n",
    "# Libraries provided by Massif project\n",
    "import constant\n",
    "import A_matrix\n",
    "import I_matrix\n",
    "import model_helper\n",
    "import gen_graph\n",
    "\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GoogLeNet (InceptionV1) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/haekyu/anaconda3/envs/massif/lib/python3.7/site-packages/lucid/misc/io/writing.py:62: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/haekyu/anaconda3/envs/massif/lib/python3.7/site-packages/lucid/misc/io/writing.py:62: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/haekyu/anaconda3/envs/massif/lib/python3.7/site-packages/lucid/misc/io/writing.py:70: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/haekyu/anaconda3/envs/massif/lib/python3.7/site-packages/lucid/misc/io/writing.py:70: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/haekyu/anaconda3/envs/massif/lib/python3.7/site-packages/lucid/misc/io/loading.py:72: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/haekyu/anaconda3/envs/massif/lib/python3.7/site-packages/lucid/misc/io/loading.py:72: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "googlenet = models.InceptionV1()\n",
    "googlenet.load_graphdef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants and hyperparameters in args:\n",
      "['gpu', 'batch_A', 'batch_I', 'layer', 'k_A', 'layer_sizes', 'layers', 'layer_blk_sizes', 'blk_headers', 'num_classes', 'img_width', 'img_height']\n"
     ]
    }
   ],
   "source": [
    "args = constant.Args\n",
    "arg_keys = [arg for arg in args.__dict__.keys() if '__' not in arg]\n",
    "\n",
    "print('Constants and hyperparameters in args:')\n",
    "print(arg_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read benign images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_dirpath = '../data/sample-images/sample-benign'\n",
    "correct_classname = 'badger'\n",
    "\n",
    "benign_imgs = []\n",
    "for benign_filename in os.listdir(benign_dirpath):\n",
    "    if 'jpg' in benign_filename:\n",
    "        benign_img = plt.imread('{}/{}'.format(benign_dirpath, benign_filename))\n",
    "        benign_imgs.append(benign_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read attacked images with different strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epss = [0.5, 1, 1.5, 2, 2.5, 3, 3.5]\n",
    "attacked_imgs = {eps: [] for eps in epss}\n",
    "\n",
    "for eps in epss:\n",
    "    attacked_dirpath = '../data/sample-images/attacked-{}'.format(eps)\n",
    "    for attacked_filename in os.listdir(attacked_dirpath):        \n",
    "        if 'npy' in attacked_filename:\n",
    "            img = np.load('{}/{}'.format(attacked_dirpath, attacked_filename))\n",
    "            attacked_imgs[eps].append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A_matrix: generate nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_matrix of the **benign** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist_A: True\n"
     ]
    }
   ],
   "source": [
    "# See if there have already been A-matrices\n",
    "A_matrix_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/A-mat'\n",
    "try:\n",
    "    exist_A = False\n",
    "    file_list = os.listdir(A_matrix_dirpath)\n",
    "    for file in file_list:\n",
    "        if 'benign' in file:\n",
    "            exist_A = True\n",
    "except:\n",
    "    exist_A = False\n",
    "print('exist_A:', exist_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read A-matrix if it exists\n",
    "if exist_A:\n",
    "    with open('{}/{}'.format(A_matrix_dirpath, 'A-benign-badger-weasel.json')) as f:\n",
    "        A_benign = json.load(f)\n",
    "    for layer in args.layers:\n",
    "        A_benign[layer] = list(map(lambda x: float(x), A_benign[layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate A-matrix if there is not any\n",
    "if not exist_A:\n",
    "    # Initialize A-matrix\n",
    "    A_benign = A_matrix.init_A_matrix_single_class(args)\n",
    "\n",
    "    # Get activation score of all neurons in all layers for all images\n",
    "    act_scores_benign = model_helper.get_all_layers_activation_score(googlenet, benign_imgs, args.layers)\n",
    "\n",
    "    # Generate A-matrix based on the activatin score\n",
    "    for layer in args.layers:\n",
    "        median_act_score_across_imgs = np.median(act_scores_benign[layer], axis=0)\n",
    "        A_benign[layer] = median_act_score_across_imgs\n",
    "\n",
    "    # Get string converted A-matrix to save it into json file\n",
    "    str_A_benign = {layer: A_benign[layer].astype(str).tolist() for layer in args.layers}\n",
    "\n",
    "    # Save the string converted A-matrix into json file\n",
    "    targeted_classname = 'weasel'\n",
    "    A_matrix_benign_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/A-mat'\n",
    "    filename = '{}/A-benign-{}-{}.json'.format(A_matrix_benign_dirpath, correct_classname, targeted_classname)\n",
    "    with open (filename, 'w') as f:\n",
    "        json.dump(str_A_benign, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_matrix of the **attacked** image with different strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read A-matrices if they exist\n",
    "if exist_A:\n",
    "    A_attacked_dict = {}\n",
    "    for eps in epss:\n",
    "        A_attacked_dict[eps] = {}\n",
    "        with open('{}/{}-{}.json'.format(A_matrix_dirpath, 'A-attacked-badger-weasel', eps)) as f:\n",
    "            A_attacked = json.load(f)\n",
    "        for layer in args.layers:\n",
    "            A_attacked_dict[eps][layer] = list(map(lambda x: float(x), A_attacked[layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate A-matrices if there are not matrices\n",
    "if not exist_A:\n",
    "    # Initialize A_attacked_dict\n",
    "    A_attacked_dict = {}\n",
    "    for eps in epss:\n",
    "        A_attacked = A_matrix.init_A_matrix_single_class(args)\n",
    "        A_attacked_dict[eps] = A_attacked\n",
    "\n",
    "    # generate A-matrix for atttacked images with different eps\n",
    "    for eps in epss:\n",
    "\n",
    "        # Get the activation scores\n",
    "        act_scores_attacked = model_helper.get_all_layers_activation_score(googlenet, attacked_imgs[eps], args.layers)\n",
    "\n",
    "        # Save the median activation scores\n",
    "        for layer in args.layers:\n",
    "            median_act_score_across_imgs = np.median(act_scores_attacked[layer], axis=0)\n",
    "            A_attacked_dict[eps][layer] = median_act_score_across_imgs\n",
    "\n",
    "    # Get string converted A-matrix to save it into json file\n",
    "    str_A_attacked_dict = {eps: {layer: A_attacked_dict[eps][layer].astype(str).tolist() for layer in args.layers} for eps in epss}\n",
    "\n",
    "    # Save the string converted A-matrix\n",
    "    A_matrix_attacked_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/A-mat'\n",
    "    for eps in epss:\n",
    "        filename = '{}/A-attacked-{}-{}-{}.json'.format(A_matrix_attacked_dirpath, correct_classname, targeted_classname, eps)\n",
    "        with open (filename, 'w') as f:\n",
    "            json.dump(str_A_attacked_dict[eps], f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top neurons from **Benign** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist_top_neurons: True\n"
     ]
    }
   ],
   "source": [
    "# See if there have already been computed top neurons\n",
    "top_neuron_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/top-neurons'\n",
    "try:\n",
    "    exist_top_neurons = False\n",
    "    file_list = os.listdir(top_neuron_dirpath)\n",
    "    for file in file_list:\n",
    "        if 'benign' in file:\n",
    "            exist_top_neurons = True\n",
    "except:\n",
    "    exist_top_neurons = False\n",
    "print('exist_top_neurons:', exist_top_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read top neurons if there have already been computed top neurons\n",
    "if exist_top_neurons:\n",
    "    file_path = '{}/{}'.format(top_neuron_dirpath, 'top-neurons-benign.json')\n",
    "    with open(file_path, 'r') as f:\n",
    "        top_neurons_benign = json.load(f)\n",
    "    for layer in args.layers:\n",
    "        top_neurons_benign[layer] = list(map(\n",
    "            lambda x: {'neuron': int(x['neuron']), 'weight': float(x['weight'])}, \n",
    "            top_neurons_benign[layer]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top neurons if there have not been computed top neurons\n",
    "if not exist_top_neurons:\n",
    "    top_p = 0.2\n",
    "    top_neurons_benign = {}\n",
    "    for layer in A_benign:\n",
    "        num = int(len(A_benign[layer]) * top_p)\n",
    "        sorted_neurons = np.argsort(-A_benign[layer])[:num]\n",
    "        top_neurons_benign[layer] = [{'neuron': str(neuron), 'weight': str(A_benign[layer][neuron])} for neuron in sorted_neurons]\n",
    "\n",
    "    file_path = '{}/{}.json'.format(top_neuron_dirpath, 'top-neurons-benign')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(top_neurons_benign, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top neurons from **Attacked** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read top neruons if there have already been computed top neurons\n",
    "# if exist_top_neurons:\n",
    "top_neurons_attacked_all_eps = {}\n",
    "\n",
    "for eps in epss:\n",
    "    top_neurons_attacked_all_eps[eps] = {}\n",
    "\n",
    "    file_path = '{}/{}-{}.json'.format(top_neuron_dirpath, 'top-neurons-attacked', eps)\n",
    "    with open(file_path, 'r') as f:\n",
    "        top_neurons_attacked = json.load(f)\n",
    "\n",
    "    for layer in args.layers:\n",
    "        top_neurons_attacked_all_eps[eps][layer] = list(map(\n",
    "            lambda x: {'neuron': int(x['neuron']), 'weight': float(x['weight'])}, \n",
    "            top_neurons_attacked[layer]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top neurons if there have not been computed top neurons\n",
    "# if not exist_top_neurons:\n",
    "top_p = 0.2\n",
    "for eps in A_attacked_dict:\n",
    "    top_neurons_attacked = {}\n",
    "    A_attacked = A_attacked_dict[eps]\n",
    "    for layer in A_attacked:\n",
    "        num = int(len(A_attacked[layer]) * top_p)\n",
    "        sorted_neurons = np.argsort(-A_attacked[layer])[:num]\n",
    "        top_neurons_attacked[layer] = [{'neuron': str(neuron), 'weight': str(A_attacked[layer][neuron])} for neuron in sorted_neurons]\n",
    "\n",
    "    graph_dir_path = '/Users/haekyu/data/massif/aggregated/badger-weasel/top-neurons'\n",
    "    file_path = '{}/{}-{}.json'.format(graph_dir_path, 'top-neurons-attacked', eps)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(top_neurons_attacked, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I_matrix: generate edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I_matrix of the **benign** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist_I: True\n"
     ]
    }
   ],
   "source": [
    "# See if there have already been I-matrices\n",
    "I_matrix_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/I-mat'\n",
    "try:\n",
    "    exist_I = False\n",
    "    file_list = os.listdir(I_matrix_dirpath)\n",
    "    for file in file_list:\n",
    "        if 'benign' in file:\n",
    "            exist_I = True\n",
    "except:\n",
    "    exist_I = False\n",
    "print('exist_I:', exist_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read I-matrix if there has already been the matrix\n",
    "if exist_I:\n",
    "    with open('{}/{}'.format(I_matrix_dirpath, 'I-benign-badger-weasel.json')) as f:\n",
    "        I_benign = json.load(f)\n",
    "    for blk in I_benign.keys():\n",
    "        I_benign[blk] = np.array(I_benign[blk], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate I-matrices if there is not any matrix\n",
    "if not exist_I:\n",
    "    # Generate I matrix for benign images\n",
    "    I_benign = I_matrix.gen_aggregated_I_matrix(args, benign_imgs, googlenet)\n",
    "\n",
    "    # Convert I matrix into string to save the matrix into json file\n",
    "    blks = list(I_benign.keys())\n",
    "    str_I_benign = {blk: I_benign[blk].astype(str).tolist() for blk in blks}\n",
    "\n",
    "    # Save the string converted I-matrix into json file\n",
    "    I_matrix_benign_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/I-mat'\n",
    "    filename = '{}/I-benign-{}-{}.json'.format(I_matrix_benign_dirpath, correct_classname, targeted_classname)\n",
    "    with open (filename, 'w') as f:\n",
    "        json.dump(str_I_benign, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I_matrix of the **attacked** images with different strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read I-matrices if there have already been the matrices\n",
    "if exist_I:\n",
    "    Is_attacked = {}\n",
    "    for eps in epss:\n",
    "        file_path = '{}/{}-{}.json'.format(I_matrix_dirpath, 'I-attacked-badger-weasel', eps)\n",
    "        with open(file_path) as f:\n",
    "            I_Attacked = json.load(f)\n",
    "            for blk in I_Attacked.keys():\n",
    "                I_Attacked[blk] = np.array(I_Attacked[blk], dtype=float)\n",
    "        Is_attacked[eps] = I_Attacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate I-matrices if there are not matrices\n",
    "if not exist_I:\n",
    "    # Initialize Is_attacked\n",
    "    Is_attacked = {}\n",
    "    for eps in epss:\n",
    "        I_attacked = I_matrix.gen_aggregated_I_matrix(args, attacked_imgs[eps], googlenet)\n",
    "        Is_attacked[eps] = I_attacked\n",
    "\n",
    "    # Save I matrix for attacked images with different eps\n",
    "    I_matrix_attacked_dirpath = '/Users/haekyu/data/massif/aggregated/badger-weasel/I-mat'\n",
    "\n",
    "    for eps in epss:\n",
    "        filename = '{}/I-attacked-{}-{}-{}.json'.format(I_matrix_attacked_dirpath, correct_classname, targeted_classname, eps)\n",
    "\n",
    "        # Convert I matrix into string\n",
    "        str_I_attacked = {blk: Is_attacked[eps][blk].astype(str).tolist() for blk in blks}\n",
    "\n",
    "        # Save the string converted I-matrix into json file\n",
    "        with open (filename, 'w') as f:\n",
    "            json.dump(str_I_attacked, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [No Need] Generate **full** graph of the **Benign** images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_need = False\n",
    "if not no_need:\n",
    "    G_benign = gen_graph.gen_full_graph(args, A_benign, I_benign)\n",
    "    G_benign_json = json_graph.node_link_data(G_benign)\n",
    "    benign_str_parsed_nodes = list(map(lambda x: {'weight': str(x['weight']), 'id': x['id']}, G_benign_json['nodes']))\n",
    "    benign_str_parsed_links = list(map(lambda x: {'source': x['source'], 'target': x['target'], 'weight': str(x['weight'])}, G_benign_json['links']))\n",
    "    G_benign_str_json = {'nodes': benign_str_parsed_nodes, 'links': benign_str_parsed_links}\n",
    "    graph_dir_path = '/Users/haekyu/data/massif/aggregated/badger-weasel/full-graph'\n",
    "    file_path = '{}/{}.json'.format(graph_dir_path, 'G-benign')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(G_benign_str_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [No Need] Generate **full** graph of the **Attacked** images with different strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_need = False\n",
    "if not no_need:\n",
    "    for eps in epss:\n",
    "        G_attacked = gen_graph.gen_full_graph(args, A_attacked_dict[eps], Is_attacked[eps])\n",
    "        G_attacked_json = json_graph.node_link_data(G_attacked)\n",
    "        attacked_str_parsed_nodes = list(map(lambda x: {'weight': str(x['weight']), 'id': x['id']}, G_attacked_json['nodes']))\n",
    "        attacked_str_parsed_links = list(map(lambda x: {'source': x['source'], 'target': x['target'], 'weight': str(x['weight'])}, G_attacked_json['links']))\n",
    "        G_attacked_str_json = {'nodes': attacked_str_parsed_nodes, 'links': attacked_str_parsed_links}\n",
    "        graph_dir_path = '/Users/haekyu/data/massif/aggregated/badger-weasel/full-graph'\n",
    "        file_path = '{}/{}-{}.json'.format(graph_dir_path, 'G-attacked', eps)\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(G_attacked_str_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate **graph** for only **top neurons**' of the **Benign** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have already computed full graph and top neurons\n",
    "graph_dir = '/Users/haekyu/data/massif/aggregated/badger-weasel/full-graph'\n",
    "file_path = '{}/{}'.format(graph_dir, 'G-benign.json')\n",
    "\n",
    "# Read full graph\n",
    "with open (file_path, 'r') as f:\n",
    "    G_benign_raw = json.load(f)\n",
    "        \n",
    "# Parse nodes    \n",
    "G_benign = {'nodes': [], 'links': []}\n",
    "node_dict = {}\n",
    "for node in G_benign_raw['nodes']:\n",
    "    layer, neuron = node['id'].split('-')\n",
    "    neuron = int(neuron)\n",
    "    top_neurons_of_layer = list(map(lambda x: x['neuron'], top_neurons_benign[layer]))\n",
    "    if neuron in top_neurons_of_layer:\n",
    "        G_benign['nodes'].append({'id': node['id'], 'weight': node['weight']})\n",
    "        node_dict[node['id']] = True\n",
    "        \n",
    "# Parse edges\n",
    "for edge in G_benign_raw['links']:\n",
    "    src, dst, w = edge['source'], edge['target'], edge['weight']\n",
    "    if (src in node_dict) and (dst in node_dict):\n",
    "        G_benign['links'].append(edge)\n",
    "        \n",
    "# Save the graph\n",
    "top_graph_dir = '/Users/haekyu/data/massif/aggregated/badger-weasel/top-graph'\n",
    "file_path = '{}/{}'.format(top_graph_dir, 'G-top-benign.json')\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(G_benign, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate **graph** for only **top neurons**' from the **Attacked** images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in epss:\n",
    "    # Read full graph\n",
    "    file_path = '{}/{}-{}.json'.format(graph_dir, 'G-attacked', eps)\n",
    "    with open (file_path, 'r') as f:\n",
    "        G_attacked_raw = json.load(f)\n",
    "        \n",
    "    # Parse nodes\n",
    "    G_attacked = {'nodes': [], 'links': []}\n",
    "    node_dict = {}\n",
    "    for node in G_attacked_raw['nodes']:\n",
    "        layer, neuron = node['id'].split('-')\n",
    "        neuron = int(neuron)\n",
    "        top_neurons_of_layer = list(map(lambda x: x['neuron'], top_neurons_attacked_all_eps[eps][layer]))\n",
    "        if neuron in top_neurons_of_layer:\n",
    "            G_attacked['nodes'].append({'id': node['id'], 'weight': node['weight']})\n",
    "            node_dict[node['id']] = True\n",
    "\n",
    "    # Parse edges\n",
    "    for edge in G_attacked_raw['links']:\n",
    "        src, dst, w = edge['source'], edge['target'], edge['weight']\n",
    "        if (src in node_dict) and (dst in node_dict):\n",
    "            G_attacked['links'].append(edge)\n",
    "\n",
    "    # Save the graph\n",
    "    top_graph_dir = '/Users/haekyu/data/massif/aggregated/badger-weasel/top-graph'\n",
    "    file_path = '{}/{}-{}.json'.format(top_graph_dir, 'G-top-attacked', eps)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(G_attacked, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "dlwp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
